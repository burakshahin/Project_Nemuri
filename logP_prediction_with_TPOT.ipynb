{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f81fd9",
   "metadata": {},
   "source": [
    "# LogP Prediction using TPOT AutoML\n",
    "## Advanced Pipeline Optimization for Experimental LogP Prediction\n",
    "\n",
    "This notebook uses TPOT (Tree-based Pipeline Optimization Tool) to automatically find the best machine learning pipeline for predicting experimental logP from calculated values.\n",
    "\n",
    "**Goals:**\n",
    "- Improve upon 80% linear regression accuracy\n",
    "- Remove 7 identified outliers (SM28, SM32, SM33, SM35, SM36, SM41, SM42)\n",
    "- Engineer meaningful molecular descriptors\n",
    "- Apply proper cross-validation to prevent overfitting\n",
    "- Use TPOT to find optimal preprocessing + model pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f9cb9",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running in fresh Colab environment)\n",
    "!pip install tpot scikit-learn pandas numpy matplotlib seaborn rdkit\n",
    "\n",
    "# Note: If rdkit installation fails in Colab, use:\n",
    "# !pip install rdkit-pypi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff93554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd52590",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the CSV file in Colab\n",
    "# Method 1: Upload from local machine\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Method 2: If you prefer, mount Google Drive and read from there\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# df = pd.read_csv('/content/drive/MyDrive/path/to/logP_data_from_ods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('logP_data_from_ods.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6456fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and data info\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623282c7",
   "metadata": {},
   "source": [
    "## 3. Remove Outliers and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outliers to remove\n",
    "outliers = ['SM28', 'SM32', 'SM33', 'SM35', 'SM36', 'SM41', 'SM42']\n",
    "\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"Outliers to remove: {outliers}\")\n",
    "\n",
    "# Remove outliers\n",
    "df_clean = df[~df['MNSol_id'].isin(outliers)].copy()\n",
    "\n",
    "print(f\"Dataset size after removing outliers: {len(df_clean)}\")\n",
    "print(f\"Removed {len(df) - len(df_clean)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5478bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing Exp_logP values (our target variable)\n",
    "df_clean = df_clean.dropna(subset=['Exp_logP'])\n",
    "\n",
    "print(f\"Dataset size after removing missing targets: {len(df_clean)}\")\n",
    "print(f\"\\nRemaining columns: {list(df_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69145ba9",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering: Create Meaningful Descriptors\n",
    "\n",
    "Beyond the simple BAR_logP, we'll create additional features that capture important molecular properties:\n",
    "- Energy differences and ratios\n",
    "- Partition behavior indicators\n",
    "- Non-linear transformations\n",
    "- Interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac025fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer meaningful molecular descriptors from BAR calculations\n",
    "    \"\"\"\n",
    "    df_feat = df.copy()\n",
    "    \n",
    "    # Original features\n",
    "    df_feat['BAR_logP'] = df['BAR_logP']\n",
    "    df_feat['BAR_Water'] = df['BAR_Water']\n",
    "    df_feat['BAR_Octanol'] = df['BAR_Octanol']\n",
    "    \n",
    "    # 1. Energy-based descriptors\n",
    "    # Total solvation energy\n",
    "    df_feat['Total_Solvation_Energy'] = df['BAR_Water'] + df['BAR_Octanol']\n",
    "    \n",
    "    # Absolute energy difference (partition strength)\n",
    "    df_feat['Energy_Difference_Abs'] = np.abs(df['BAR_Water'] - df['BAR_Octanol'])\n",
    "    \n",
    "    # Solvation ratio (avoids division by zero)\n",
    "    df_feat['Solvation_Ratio'] = df['BAR_Water'] / (df['BAR_Octanol'] + 1e-6)\n",
    "    \n",
    "    # 2. Non-linear transformations\n",
    "    # These can capture non-linear relationships\n",
    "    df_feat['BAR_logP_squared'] = df['BAR_logP'] ** 2\n",
    "    df_feat['BAR_logP_cubed'] = df['BAR_logP'] ** 3\n",
    "    df_feat['BAR_logP_sqrt'] = np.sqrt(np.abs(df['BAR_logP']))  * np.sign(df['BAR_logP'])\n",
    "    \n",
    "    # Log transformations (shifted to avoid log(0))\n",
    "    df_feat['log_BAR_Water'] = np.log(np.abs(df['BAR_Water']) + 1) * np.sign(df['BAR_Water'])\n",
    "    df_feat['log_BAR_Octanol'] = np.log(np.abs(df['BAR_Octanol']) + 1) * np.sign(df['BAR_Octanol'])\n",
    "    \n",
    "    # 3. Partition behavior indicators\n",
    "    # Hydrophilicity indicator\n",
    "    df_feat['Hydrophilic_Score'] = df['BAR_Water'] / (np.abs(df['BAR_Water']) + np.abs(df['BAR_Octanol']) + 1e-6)\n",
    "    \n",
    "    # Lipophilicity indicator\n",
    "    df_feat['Lipophilic_Score'] = np.abs(df['BAR_Octanol']) / (np.abs(df['BAR_Water']) + np.abs(df['BAR_Octanol']) + 1e-6)\n",
    "    \n",
    "    # 4. Interaction terms\n",
    "    df_feat['Water_Octanol_Product'] = df['BAR_Water'] * df['BAR_Octanol']\n",
    "    \n",
    "    # 5. Linear correction from your Excel formula: y = 0.8645x - 0.1688\n",
    "    df_feat['Linear_Corrected_logP'] = 0.8645 * df['BAR_logP'] - 0.1688\n",
    "    \n",
    "    # 6. Deviation from linear prediction\n",
    "    df_feat['Linear_Deviation'] = df['BAR_logP'] - df_feat['Linear_Corrected_logP']\n",
    "    \n",
    "    # 7. Energy normalized by logP\n",
    "    df_feat['Energy_per_logP'] = df_feat['Total_Solvation_Energy'] / (np.abs(df['BAR_logP']) + 1e-6)\n",
    "    \n",
    "    # 8. Categorize molecules by water solubility\n",
    "    df_feat['High_Water_Solubility'] = (df['BAR_Water'] > df['BAR_Water'].median()).astype(int)\n",
    "    df_feat['High_Octanol_Solubility'] = (df['BAR_Octanol'] < df['BAR_Octanol'].median()).astype(int)\n",
    "    \n",
    "    return df_feat\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_features(df_clean)\n",
    "\n",
    "print(f\"Number of engineered features: {len(df_features.columns)}\")\n",
    "print(f\"\\nNew features created:\")\n",
    "new_features = [col for col in df_features.columns if col not in df_clean.columns]\n",
    "for i, feat in enumerate(new_features, 1):\n",
    "    print(f\"{i}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017ea1b",
   "metadata": {},
   "source": [
    "## 5. Prepare Features and Target for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67413ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "feature_columns = [col for col in df_features.columns \n",
    "                   if col not in ['MNSol_id', 'Name', 'Exp_Water', 'Exp_Octanol', 'Exp_logP']]\n",
    "\n",
    "X = df_features[feature_columns].values\n",
    "y = df_features['Exp_logP'].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFeatures used for modeling:\")\n",
    "for i, feat in enumerate(feature_columns, 1):\n",
    "    print(f\"{i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e795f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining NaN or infinite values\n",
    "print(\"Checking for invalid values...\")\n",
    "print(f\"NaN values in X: {np.isnan(X).sum()}\")\n",
    "print(f\"Infinite values in X: {np.isinf(X).sum()}\")\n",
    "print(f\"NaN values in y: {np.isnan(y).sum()}\")\n",
    "\n",
    "# Replace any inf values with large finite numbers\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "print(\"\\nData cleaning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337ade9",
   "metadata": {},
   "source": [
    "## 6. Split Data for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23647490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Test set proportion: {X_test.shape[0] / len(X) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c962c3ea",
   "metadata": {},
   "source": [
    "## 7. Baseline: Linear Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc753b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit simple linear regression as baseline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create baseline pipeline with scaling + ridge regression\n",
    "baseline_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODEL PERFORMANCE (Ridge Regression)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"R¬≤ Score: {r2_baseline:.4f} ({r2_baseline*100:.2f}% accuracy)\")\n",
    "print(f\"RMSE: {rmse_baseline:.4f}\")\n",
    "print(f\"MAE: {mae_baseline:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cross-validation score\n",
    "cv_scores = cross_val_score(baseline_model, X_train, y_train, cv=5, \n",
    "                            scoring='r2', n_jobs=-1)\n",
    "print(f\"\\nCross-validation R¬≤ scores: {cv_scores}\")\n",
    "print(f\"Mean CV R¬≤: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d59297",
   "metadata": {},
   "source": [
    "## 8. TPOT: Automated Machine Learning Pipeline Optimization\n",
    "\n",
    "TPOT will automatically:\n",
    "- Try different preprocessing methods (scaling, feature selection, PCA, etc.)\n",
    "- Test multiple regression algorithms (Random Forest, XGBoost, SVR, etc.)\n",
    "- Optimize hyperparameters\n",
    "- Find the best pipeline combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7e3a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "\n",
    "   \"# Configure TPOT for comprehensive search\\n\",\n",
    "    \"# MAXIMUM THOROUGHNESS SETTINGS for best possible results\\n\",\n",
    "    \"# This will take 6-8 hours but will find the absolute best pipeline\\n\",\n",
    "    \"\\n\",\n",
    "    \"tpot_config = {\\n\",\n",
    "    \"    'generations': 150,  # Maximum generations for exhaustive exploration\\n\",\n",
    "    \"    'population_size': 100,  # Large population for maximum diversity\\n\",\n",
    "    \"    'cv': 5,  # 5-fold cross-validation (good for ~100 samples)\\n\",\n",
    "    \"    'random_state': 42,\\n\",\n",
    "    \"    'verbosity': 2,  # Show progress\\n\",\n",
    "    \"    'scoring': 'r2',  # Optimize for R¬≤ score\\n\",\n",
    "    \"    'n_jobs': -1,  # Use all available cores\\n\",\n",
    "    \"    'max_time_mins': 480,  # 8 hours - maximum thoroughness\\n\",\n",
    "    \"    'max_eval_time_mins': 10,  # Allow more time for complex pipelines\\n\",\n",
    "    \"    'early_stop': 20,  # More patience before stopping (20 generations)\\n\",\n",
    "    \"}\"\n",
    "\n",
    "print(\"TPOT Configuration:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in tpot_config.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"\\\\nüöÄ MAXIMUM THOROUGHNESS MODE ACTIVATED!\\\")\\n\",\n",
    "    \"print(\\\"Expected runtime: ~6-8 hours (480 minutes max)\\\")\\n\",\n",
    "    \"print(\\\"This will evaluate 15,000+ pipeline configurations\\\")\\n\",\n",
    "    \"print(\\\"With 5-fold CV: ~75,000 total model fits\\\")\\n\",\n",
    "    \"print(\\\"\\\\n‚ö†Ô∏è  IMPORTANT: Make sure Google Colab won't timeout!\\\")\\n\",\n",
    "    \"print(\\\"Tip: Keep the browser tab active or use Colab Pro for longer runtimes\\\")\\n\",\n",
    "    \"print(\\\"\\\\nFor faster testing: reduce to generations=20, population_size=20, max_time_mins=30\\\")\\n\",\n",
    "    \"print(\\\"Current settings will find the ABSOLUTE BEST pipeline!\\\\n\\\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ad8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TPOT\n",
    "tpot = TPOTRegressor(**tpot_config)\n",
    "\n",
    "print(\"Starting TPOT optimization...\")\n",
    "print(\"This may take a while. Please be patient!\\n\")\n",
    "\n",
    "# Fit TPOT\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TPOT OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fcd0c0",
   "metadata": {},
   "source": [
    "## 9. Evaluate TPOT Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with TPOT model\n",
    "y_pred_tpot = tpot.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "r2_tpot = r2_score(y_test, y_pred_tpot)\n",
    "rmse_tpot = np.sqrt(mean_squared_error(y_test, y_pred_tpot))\n",
    "mae_tpot = mean_absolute_error(y_test, y_pred_tpot)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TPOT MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"R¬≤ Score: {r2_tpot:.4f} ({r2_tpot*100:.2f}% accuracy)\")\n",
    "print(f\"RMSE: {rmse_tpot:.4f}\")\n",
    "print(f\"MAE: {mae_tpot:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare with baseline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: TPOT vs BASELINE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"R¬≤ Improvement: {(r2_tpot - r2_baseline):.4f} ({(r2_tpot - r2_baseline)*100:.2f}%)\")\n",
    "print(f\"RMSE Improvement: {(rmse_baseline - rmse_tpot):.4f}\")\n",
    "print(f\"MAE Improvement: {(mae_baseline - mae_tpot):.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if r2_tpot > r2_baseline:\n",
    "    print(\"\\n‚úÖ TPOT model outperforms baseline!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Baseline still competitive. Consider longer TPOT runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d2ef9",
   "metadata": {},
   "source": [
    "## 10. Cross-Validation Analysis (Preventing Overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform detailed cross-validation analysis\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    tpot.fitted_pipeline_,  # Use the best pipeline found by TPOT\n",
    "    X_train, \n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring=['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"5-FOLD CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# R¬≤ scores\n",
    "train_r2 = cv_results['train_r2']\n",
    "test_r2 = cv_results['test_r2']\n",
    "\n",
    "print(f\"\\nR¬≤ Score:\")\n",
    "print(f\"  Training:   {train_r2.mean():.4f} (+/- {train_r2.std() * 2:.4f})\")\n",
    "print(f\"  Validation: {test_r2.mean():.4f} (+/- {test_r2.std() * 2:.4f})\")\n",
    "\n",
    "# RMSE\n",
    "train_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\n",
    "test_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n",
    "\n",
    "print(f\"\\nRMSE:\")\n",
    "print(f\"  Training:   {train_rmse.mean():.4f} (+/- {train_rmse.std() * 2:.4f})\")\n",
    "print(f\"  Validation: {test_rmse.mean():.4f} (+/- {test_rmse.std() * 2:.4f})\")\n",
    "\n",
    "# MAE\n",
    "train_mae = -cv_results['train_neg_mean_absolute_error']\n",
    "test_mae = -cv_results['test_neg_mean_absolute_error']\n",
    "\n",
    "print(f\"\\nMAE:\")\n",
    "print(f\"  Training:   {train_mae.mean():.4f} (+/- {train_mae.std() * 2:.4f})\")\n",
    "print(f\"  Validation: {test_mae.mean():.4f} (+/- {test_mae.std() * 2:.4f})\")\n",
    "\n",
    "# Overfitting check\n",
    "overfit_indicator = train_r2.mean() - test_r2.mean()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Overfitting Indicator: {overfit_indicator:.4f}\")\n",
    "if overfit_indicator < 0.1:\n",
    "    print(\"‚úÖ Model generalizes well! Low overfitting.\")\n",
    "elif overfit_indicator < 0.2:\n",
    "    print(\"‚ö†Ô∏è  Moderate overfitting detected.\")\n",
    "else:\n",
    "    print(\"‚ùå High overfitting! Model may not generalize well.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97339554",
   "metadata": {},
   "source": [
    "## 11. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef30655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Predicted vs Actual (TPOT)\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(y_test, y_pred_tpot, alpha=0.6, s=100, edgecolors='k', linewidth=0.5)\n",
    "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('Experimental logP', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Predicted logP (TPOT)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'TPOT Model: R¬≤ = {r2_tpot:.4f}', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Predicted vs Actual (Baseline)\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(y_test, y_pred_baseline, alpha=0.6, s=100, \n",
    "            edgecolors='k', linewidth=0.5, color='orange')\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "ax2.set_xlabel('Experimental logP', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Predicted logP (Baseline)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Baseline Model: R¬≤ = {r2_baseline:.4f}', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals Plot (TPOT)\n",
    "ax3 = axes[1, 0]\n",
    "residuals_tpot = y_test - y_pred_tpot\n",
    "ax3.scatter(y_pred_tpot, residuals_tpot, alpha=0.6, s=100, edgecolors='k', linewidth=0.5)\n",
    "ax3.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax3.set_xlabel('Predicted logP', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Residuals', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('TPOT Residuals Plot', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Model Comparison\n",
    "ax4 = axes[1, 1]\n",
    "models = ['Baseline\\n(Ridge)', 'TPOT\\n(Optimized)']\n",
    "r2_scores = [r2_baseline, r2_tpot]\n",
    "colors = ['orange', 'steelblue']\n",
    "\n",
    "bars = ax4.bar(models, r2_scores, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax4.set_ylabel('R¬≤ Score', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylim([0, 1])\n",
    "ax4.axhline(y=0.8, color='green', linestyle='--', lw=2, label='80% Target')\n",
    "ax4.legend(fontsize=10)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, r2_scores):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{score:.4f}\\n({score*100:.2f}%)',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('logP_prediction_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization saved as 'logP_prediction_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8dbd4",
   "metadata": {},
   "source": [
    "## 12. Export Optimized Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf250d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the best pipeline as Python code\n",
    "tpot.export('tpot_logP_pipeline.py')\n",
    "\n",
    "print(\"‚úÖ Best pipeline exported to 'tpot_logP_pipeline.py'\")\n",
    "print(\"\\nYou can use this pipeline in production without TPOT!\\n\")\n",
    "\n",
    "# Display the pipeline\n",
    "print(\"=\"*60)\n",
    "print(\"BEST PIPELINE FOUND BY TPOT:\")\n",
    "print(\"=\"*60)\n",
    "print(tpot.fitted_pipeline_)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a085b0a7",
   "metadata": {},
   "source": [
    "## 13. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e310acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to extract feature importances if available\n",
    "try:\n",
    "    # Get the final estimator from the pipeline\n",
    "    final_estimator = None\n",
    "    \n",
    "    # Check if fitted_pipeline has named_steps (sklearn Pipeline)\n",
    "    if hasattr(tpot.fitted_pipeline_, 'named_steps'):\n",
    "        steps = list(tpot.fitted_pipeline_.named_steps.values())\n",
    "        final_estimator = steps[-1]\n",
    "    elif hasattr(tpot.fitted_pipeline_, 'steps'):\n",
    "        final_estimator = tpot.fitted_pipeline_.steps[-1][1]\n",
    "    \n",
    "    # Extract feature importances\n",
    "    if final_estimator is not None and hasattr(final_estimator, 'feature_importances_'):\n",
    "        importances = final_estimator.feature_importances_\n",
    "        \n",
    "        # Create dataframe\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_columns,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FEATURE IMPORTANCE RANKING\")\n",
    "        print(\"=\"*60)\n",
    "        print(feature_importance_df.to_string(index=False))\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Plot top 15 features\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = feature_importance_df.head(15)\n",
    "        plt.barh(range(len(top_features)), top_features['Importance'], \n",
    "                color='steelblue', edgecolor='black', linewidth=1)\n",
    "        plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "        plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "        plt.title('Top 15 Most Important Features', fontsize=14, fontweight='bold')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nüìä Feature importance plot saved as 'feature_importance.png'\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Selected model doesn't support feature importance extraction.\")\n",
    "        print(\"This is common for some models like SVR or neural networks.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not extract feature importances: {str(e)}\")\n",
    "    print(\"This is normal for some pipeline configurations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1574fafa",
   "metadata": {},
   "source": [
    "## 14. Save Model for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15476ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the entire TPOT fitted pipeline\n",
    "joblib.dump(tpot.fitted_pipeline_, 'tpot_logP_model.pkl')\n",
    "\n",
    "print(\"‚úÖ Model saved as 'tpot_logP_model.pkl'\")\n",
    "print(\"\\nTo load and use the model later:\")\n",
    "print(\"\"\"\\n# Load model\n",
    "import joblib\n",
    "model = joblib.load('tpot_logP_model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_new)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34106d1f",
   "metadata": {},
   "source": [
    "## 15. Make Predictions on New Data (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create predictions for all data\n",
    "X_all = df_features[feature_columns].values\n",
    "X_all = np.nan_to_num(X_all, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "y_all_pred = tpot.fitted_pipeline_.predict(X_all)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_results = df_features[['MNSol_id', 'Name', 'BAR_logP', 'Exp_logP']].copy()\n",
    "df_results['Predicted_logP'] = y_all_pred\n",
    "df_results['Prediction_Error'] = df_results['Exp_logP'] - df_results['Predicted_logP']\n",
    "df_results['Absolute_Error'] = np.abs(df_results['Prediction_Error'])\n",
    "\n",
    "# Sort by absolute error\n",
    "df_results_sorted = df_results.sort_values('Absolute_Error')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST PREDICTIONS (Top 10 with lowest error)\")\n",
    "print(\"=\"*80)\n",
    "print(df_results_sorted.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORST PREDICTIONS (Top 10 with highest error)\")\n",
    "print(\"=\"*80)\n",
    "print(df_results_sorted.tail(10).to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "df_results_sorted.to_csv('logP_predictions_detailed.csv', index=False)\n",
    "print(\"\\n‚úÖ Detailed predictions saved to 'logP_predictions_detailed.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57745ba",
   "metadata": {},
   "source": [
    "## 16. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c53d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE METRICS:\")\n",
    "print(f\"   Baseline R¬≤: {r2_baseline:.4f} ({r2_baseline*100:.2f}%)\")\n",
    "print(f\"   TPOT R¬≤:     {r2_tpot:.4f} ({r2_tpot*100:.2f}%)\")\n",
    "print(f\"   Improvement: {(r2_tpot - r2_baseline)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüéØ TARGET ACHIEVEMENT:\")\n",
    "if r2_tpot >= 0.80:\n",
    "    print(f\"   ‚úÖ SUCCESS! Exceeded 80% accuracy target\")\n",
    "    print(f\"   Achieved: {r2_tpot*100:.2f}%\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Close to target: {r2_tpot*100:.2f}%\")\n",
    "    print(f\"   Need: {(0.80-r2_tpot)*100:.2f}% more\")\n",
    "\n",
    "print(f\"\\nüî¨ DATA QUALITY:\")\n",
    "print(f\"   Total samples: {len(df_clean)}\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "print(f\"   Features engineered: {len(feature_columns)}\")\n",
    "\n",
    "print(f\\\"\\\\nüìà RECOMMENDATIONS FOR FURTHER IMPROVEMENT:\\\")\\n\",\n",
    "    \"print(\\\"\\\"\\\"\\\\n   1. ‚úÖ MAXIMUM TPOT (150 gen, 100 pop, 480 min) - Already configured!\\n\",\n",
    "    \"   2. ‚úÖ This is the most thorough configuration possible!\\n\",\n",
    "    \"   3. If results still need improvement, consider:\\n\",\n",
    "    \"      ‚Ä¢ Ensemble methods (stacking multiple TPOT runs with different seeds)\\n\",\n",
    "    \"      ‚Ä¢ Add more chemical descriptors (molecular weight, rotatable bonds,\\n\",\n",
    "    \"        TPSA, aromatic rings, H-bond donors/acceptors, LogS, etc.)\\n\",\n",
    "    \"      ‚Ä¢ Use RDKit to calculate 200+ molecular descriptors from SMILES\\n\",\n",
    "    \"      ‚Ä¢ Collect more training data (more samples = better generalization)\\n\",\n",
    "    \"   4. Try different search spaces after this run:\\n\",\n",
    "    \"      ‚Ä¢ 'gradient-boosting' for tree-based models\\n\",\n",
    "    \"      ‚Ä¢ 'neural-network' for deep learning approaches\\n\",\n",
    "    \"   5. Investigate systematic errors in predictions\\n\",\n",
    "    \"   6. Consider domain-specific corrections based on chemical classes\\n\",\n",
    "    \"\\\"\\\"\\\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚úÖ Analysis complete! All results and models saved.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa36b3f",
   "metadata": {},
   "source": [
    "## Optional: Advanced TPOT Configuration\n",
    "\n",
    "If you want to further optimize, you can try these advanced configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9624e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced TPOT configuration with custom search space\n",
    "# Uncomment and run this cell if you want to try different configurations\n",
    "\n",
    "\"\"\"\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "# Option 1: Use a lighter search space for faster optimization\n",
    "tpot_light = TPOTRegressor(\n",
    "    search_space='linear-light',  # Focus on linear models\n",
    "    generations=30,\n",
    "    population_size=30,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    verbosity=2,\n",
    "    n_jobs=-1,\n",
    "    max_time_mins=30\n",
    ")\n",
    "\n",
    "# Option 2: Use gradient boosting models\n",
    "tpot_gradient = TPOTRegressor(\n",
    "    search_space='gradient-boosting',  # Focus on gradient boosting\n",
    "    generations=40,\n",
    "    population_size=40,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    verbosity=2,\n",
    "    n_jobs=-1,\n",
    "    max_time_mins=45\n",
    ")\n",
    "\n",
    "# Option 3: Full search with longer runtime (recommended for best results)\n",
    "tpot_full = TPOTRegressor(\n",
    "    generations=100,\n",
    "    population_size=100,\n",
    "    cv=10,  # More folds for better validation\n",
    "    random_state=42,\n",
    "    verbosity=2,\n",
    "    n_jobs=-1,\n",
    "    max_time_mins=240,  # 4 hours\n",
    "    early_stop=15\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Advanced configurations available in comments above.\")\n",
    "print(\"Uncomment and run to try different TPOT strategies.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
